# DAT 20 NYC Assignment 3

For assignment 3, I choose to write about the “Naive Bayes” algorithm for the Classification technique.   A classification technique is used to predict a specific outcome.  I choose this because I have used and am familiar with logistic regressions for classification problems in my work as a portfolio manager.  Logistic regressions are typically used to predict outcomes such as prepayments (on portfolios of mortgages), or defaults (on portfolios of consumer or other granular credits).   I have read about Naive Bayes frequently as a classification techniques and would like to learn more about it.  Its implementation appears to be ideally suited to big data problems. 

Naive Bayes uses Bayes’ Theorem, which states the the conditional probability of event B given event A can be expressed as the probability of both B & A occurring, divided by the probability of event A occurring:

** 	Prob(B|A) = Prob (A u B)/Prob(B)**

The Naive Bayes algorithm is said to be fast and scalable and lends itself to parallel execution.  The calculation of probabilities is a straightforward exercise where outcomes in each category can be determined by a counting algorithm.    Therefore, it is very good for large datasets.  

A Naive Bayes technique can be used in many different ways:

### **i) What credit card borrower characteristics are the most predictive of default?**   

For example, you can break down a population of credit card borrowers by income and calculate the conditional probability of default for each particular income ‘bracket.’As well, one could calculate the conditional probability of default over a number of different borrower characteristics.  This is particularly useful in investing in structured credit or any other portfolio strategy.  

**ii) Detecting causes of disease in a large population

One could test a population and relationships between characteristics of people who contract a disease and those who do not.   

**iii) 

### **NEW WORDS:**
* Support Vector Machine
* Minimum Description Length
* One-Class Support Vector Machine
* Enhanced K-Means
* Orthogonal Partitioning Clustering
* Expectation Maximization
* Non-negative Matrix Factorization
* Principal Components Analysis
* Singular Vector Decomposition

